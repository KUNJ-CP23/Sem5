{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1422b500",
   "metadata": {},
   "source": [
    "# Apriori Algorithm Implementation Assignment\n",
    "\n",
    "### Objective:\n",
    "You will implement the **Apriori algorithm** from scratch (i.e., without using any libraries like `mlxtend`) to find frequent itemsets and generate association rules.\n",
    "\n",
    "### Dataset:\n",
    "Use the [Online Retail Dataset](https://www.kaggle.com/datasets/vijayuv/onlineretail) from Kaggle. You can filter it for a specific country (e.g., `United Kingdom`) and time range to reduce size if needed.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70f3d67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85128a0",
   "metadata": {},
   "source": [
    "## Step 1: Data Preprocessing\n",
    "\n",
    "- Load the dataset\n",
    "- Remove rows with missing values\n",
    "- Filter out rows where `Quantity <= 0`\n",
    "- Convert Data into Basket Format\n",
    "\n",
    "ðŸ‘‰ **Implement code below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd66a2e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kunjp\\AppData\\Local\\Temp\\ipykernel_11956\\2429339532.py:16: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  basket = basket.applymap(lambda x: 1 if x > 0 else 0)\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv(r\"C:\\Ddrive data\\Sem 5 Labs\\Data Mining\\ProjectTasks\\Datasets\\OnlineRetail.csv\", encoding='ISO-8859-1')\n",
    "# Preprocess as per the instructions above | We have already done in TASK 2\n",
    "\n",
    "# Remove missing values\n",
    "df.dropna(subset=[\"InvoiceNo\", \"Description\"], inplace=True)\n",
    "\n",
    "# Filter for Quantity <= 0\n",
    "df = df[df[\"Quantity\"] > 0]\n",
    "\n",
    "# Filter for a specific country (United Kingdom)\n",
    "df = df[df[\"Country\"] == \"United Kingdom\"]\n",
    "\n",
    "# for Basket\n",
    "basket = df.groupby(['InvoiceNo', 'Description'])['Quantity'].sum().unstack().reset_index().fillna(0).set_index('InvoiceNo')\n",
    "basket = basket.applymap(lambda x: 1 if x > 0 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a37baf6",
   "metadata": {},
   "source": [
    "## Step 2: Implement Apriori Algorithm\n",
    "Step-by-Step Procedure:\n",
    "1. Generate Frequent 1-Itemsets\n",
    "Count the frequency (support) of each individual item in the dataset.\n",
    "Keep only those with support â‰¥ min_support.\n",
    "â†’ Result is L1 (frequent 1-itemsets)\n",
    "2. Iterative Candidate Generation (k = 2 to n)\n",
    "While L(k-1) is not empty:\n",
    "a. Candidate Generation\n",
    "\n",
    "Generate candidate itemsets Ck of size k from L(k-1) using the Apriori property:\n",
    "Any (k-itemset) is only frequent if all of its (kâˆ’1)-subsets are frequent.\n",
    "b. Prune Candidates\n",
    "Eliminate candidates that have any (kâˆ’1)-subset not in L(k-1).\n",
    "c. Count Support\n",
    "For each transaction, count how many times each candidate in Ck appears.\n",
    "d. Generate Frequent Itemsets\n",
    "Form Lk by keeping candidates from Ck that meet the min_support.\n",
    "Repeat until Lk becomes empty.\n",
    "Implement the following functions:\n",
    "1. `get_frequent_itemsets(transactions, min_support)` - Returns frequent itemsets and their support\n",
    "2. `generate_candidates(prev_frequent_itemsets, k)` - Generates candidate itemsets of length `k`\n",
    "3. `calculate_support(transactions, candidates)` - Calculates the support count for each candidate\n",
    "\n",
    "**Write reusable functions** for each part of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f9310a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "\n",
    "# ---------------------------\n",
    "# Helper (tiny, for readability)\n",
    "# ---------------------------\n",
    "def _min_count(min_support, n_tx):\n",
    "    \"\"\"\n",
    "    Accept either absolute support (>=1) or relative support (0< s <=1).\n",
    "    Returns the minimum support count as an integer.\n",
    "    \"\"\"\n",
    "    return min_support if min_support >= 1 else ceil(min_support * n_tx)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 1) calculate_support(transactions, candidates)\n",
    "# ------------------------------------------------------\n",
    "def calculate_support(transactions, candidates):\n",
    "    \"\"\"\n",
    "    Count support *counts* for each candidate itemset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    transactions : list[set]\n",
    "        Each transaction is a Python set of items.\n",
    "    candidates : list[frozenset]\n",
    "        Candidate itemsets to count.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict[frozenset, int]\n",
    "        Support *count* of each candidate.\n",
    "    \"\"\"\n",
    "    # Initialize counts to zero for every candidate\n",
    "    counts = {c: 0 for c in candidates}\n",
    "\n",
    "    # For every transaction, increment counts of candidates contained in it\n",
    "    for tx in transactions:\n",
    "        for c in candidates:\n",
    "            # c âŠ† tx ?  (Python: c.issubset(tx))\n",
    "            if c.issubset(tx):\n",
    "                counts[c] += 1\n",
    "\n",
    "    return counts\n",
    "\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 2) generate_candidates(prev_frequent_itemsets, k)\n",
    "# ------------------------------------------------------\n",
    "def generate_candidates(prev_frequent_itemsets, k):\n",
    "    \"\"\"\n",
    "    JOIN + PRUNE to build k-item candidates from (k-1)-item frequent sets,\n",
    "    **without** using itertools.combinations.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    prev_frequent_itemsets : iterable[frozenset]\n",
    "        Frequent itemsets of size (k-1).\n",
    "    k : int\n",
    "        Target candidate size.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[frozenset]\n",
    "        Candidate k-itemsets that pass the Apriori prune.\n",
    "    \"\"\"\n",
    "    prev_list = sorted([tuple(sorted(fs)) for fs in prev_frequent_itemsets])\n",
    "    prev_set  = set(prev_frequent_itemsets)  # for O(1) membership tests\n",
    "\n",
    "    candidates = []\n",
    "\n",
    "    # --- JOIN step ---\n",
    "    # Pairwise join itemsets that share the same first k-2 items.\n",
    "    # Because prev_list is lexicographically sorted, we can compare prefixes.\n",
    "    for i in range(len(prev_list)):\n",
    "        for j in range(i + 1, len(prev_list)):\n",
    "            # If prefixes differ, no further j will match (due to sorting)\n",
    "            if prev_list[i][:k-2] != prev_list[j][:k-2]:\n",
    "                break\n",
    "\n",
    "            # Union the two (k-1)-item sets â†’ at most k items\n",
    "            union_set = frozenset(prev_list[i]) | frozenset(prev_list[j])\n",
    "\n",
    "            # Keep only unions that actually grew to size k\n",
    "            if len(union_set) != k:\n",
    "                continue\n",
    "\n",
    "            # --- PRUNE step (Apriori property) ---\n",
    "            # Every (k-1)-subset of this k-candidate must be frequent.\n",
    "            # Generate (k-1)-subsets *without* combinations:\n",
    "            # remove each element once and check membership.\n",
    "            all_subsets_frequent = True\n",
    "            for x in union_set:\n",
    "                if frozenset(union_set - {x}) not in prev_set:\n",
    "                    all_subsets_frequent = False\n",
    "                    break\n",
    "\n",
    "            if all_subsets_frequent and union_set not in candidates:\n",
    "                candidates.append(union_set)\n",
    "\n",
    "    return candidates\n",
    "\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 3) get_frequent_itemsets(transactions, min_support)\n",
    "# ------------------------------------------------------\n",
    "def get_frequent_itemsets(transactions, min_support):\n",
    "    \"\"\"\n",
    "    Full Apriori loop:\n",
    "      - Build L1 (frequent 1-itemsets)\n",
    "      - For k = 2.., iteratively create Ck (candidates), count support, prune to Lk\n",
    "      - Stop when no more frequent itemsets\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    transactions : list[iterable]\n",
    "        Raw transactions; each is an iterable of items. (Will be converted to sets.)\n",
    "    min_support : float|int\n",
    "        If >=1: absolute count. If 0<value<=1: relative min support (fraction).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict[frozenset, int]\n",
    "        All frequent itemsets with their support *counts*.\n",
    "    \"\"\"\n",
    "    # Normalize transactions as list[set]\n",
    "    tx = [set(t) for t in transactions]\n",
    "    n_tx = len(tx)\n",
    "    min_cnt = _min_count(min_support, n_tx)\n",
    "\n",
    "    frequent = {}  # accumulate all Lk across levels\n",
    "\n",
    "    # ---------- L1: frequent 1-itemsets ----------\n",
    "    # Count singletons\n",
    "    singleton_counts = {}\n",
    "    for t in tx:\n",
    "        for item in t:\n",
    "            singleton_counts[item] = singleton_counts.get(item, 0) + 1\n",
    "\n",
    "    Lk = {frozenset([i]): c for i, c in singleton_counts.items() if c >= min_cnt}\n",
    "    frequent.update(Lk)\n",
    "\n",
    "    k = 2\n",
    "    # ---------- Iterate for k >= 2 ----------\n",
    "    while Lk:\n",
    "        # a) Candidate generation: Ck from L(k-1)\n",
    "        Ck = generate_candidates(Lk.keys(), k)\n",
    "\n",
    "        if not Ck:\n",
    "            break\n",
    "\n",
    "        # b) Count support for candidates\n",
    "        Ck_counts = calculate_support(tx, Ck)\n",
    "\n",
    "        # c) Prune to Lk (keep those that meet min support)\n",
    "        Lk = {c: cnt for c, cnt in Ck_counts.items() if cnt >= min_cnt}\n",
    "\n",
    "        # d) Accumulate and move to next level\n",
    "        frequent.update(Lk)\n",
    "        k += 1\n",
    "\n",
    "    return frequent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb8c0fe",
   "metadata": {},
   "source": [
    "## Step 3: Generate Association Rules\n",
    "\n",
    "- Use frequent itemsets to generate association rules\n",
    "- For each rule `A => B`, calculate:\n",
    "  - **Support**\n",
    "  - **Confidence**\n",
    "- Only return rules that meet a minimum confidence threshold (e.g., 0.5)\n",
    "\n",
    "ðŸ‘‰ **Implement rule generation function below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bc236e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0cf26889",
   "metadata": {},
   "source": [
    "## Step 4: Output and Visualize\n",
    "\n",
    "- Print top 10 frequent itemsets\n",
    "- Print top 10 association rules (by confidence or lift)\n",
    "\n",
    "ðŸ‘‰ **Output results below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3443a37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd65f4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
